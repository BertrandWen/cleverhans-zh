# Privacy and machine learning: two unexpected allies?

Apr 29, 2018

by Nicolas Papernot and Ian Goodfellow

在许多机器学习的应用中，比如将机器学习用于医疗诊断，我们都希望机器学习算法不要去记录训练集中的一些敏感信息，比如说一个患者个体特定的医疗历史。*差分隐私*就是这样一个框架来衡量一个算法提供的隐私保证。透过差分隐私，我们能够设计出机器学习算法，其能够可靠地在隐私数据上训练。我们（和 Martín Abadi, Úlfar Erlingsson, Ilya Mironov, Ananth Raghunathan, Shuang Song 以及 Kunal Talwar 一起） 在机器学习中的差分隐私上的工作，使得机器学习行业的研究者更容易在这个方向上做出贡献（即使是那些不精通差分隐私中的数学的人）。在这篇博文中，我们将展示怎么做到这一点。

最关机的一系列算法被命名为 Private Aggregation of Teacher Ensembles (PATE)。在 PATE 框架中的一个最重要的之处，除了它的名字外，就是每个知道怎么训练一个有监督机器学习模型（比如说一个神经网络）的人都能为机器学习上的差分隐私研究做出贡献。PATE 通过精心协调几个不同的机器学习模型的活动来实现隐私学习。只要你遵循 PATE 框架的的流程，整个模型结果就将有可量化的隐私保证。每个独立的机器模型都是用普通的有监督学习技巧训练的，而我们大多数读者可能已经对怎么攻击 ImageNet 的分类或其他传统的机器学习方向很熟悉了。

如有有人能够在 PATE 下为单个模型设计出更好的架构，或者说更好的训练算法，那么他们也能改进监督学习本身（并不只是个人的分类结果）。实际上，差分隐私可以被看成是一些从业者在普遍遇到的问题上的一个约束，即使在不要求差分隐私的设定下也是这样。这里面就包括过拟合。我们会在这篇博客中陈述在隐私和学习二者之前有趣的联系。特别的是，我们最近提出了扩展的 PATE，其改进了怎么协调不同的机器学习模型，从而能同时提高 PATE 框架得到的模型的准确性和隐私性。这表明了差分隐私的目标和学习模型的目标是一致的。

## 我们为什么需要隐私机器学习算法？