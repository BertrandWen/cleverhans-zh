# Privacy and machine learning: two unexpected allies?

Apr 29, 2018

by Nicolas Papernot and Ian Goodfellow

在许多机器学习的应用中，比如将机器学习用于医疗诊断，我们都希望机器学习算法不要去记录训练集中的一些敏感信息，比如说一个患者个体特定的医疗历史。*差分隐私*就是这样一个框架来衡量一个算法提供的隐私保证。透过差分隐私，我们能够设计出机器学习算法，其能够可靠地在隐私数据上训练。我们（和 Martín Abadi, Úlfar Erlingsson, Ilya Mironov, Ananth Raghunathan, Shuang Song 以及 Kunal Talwar 一起） 在机器学习中的差分隐私上的工作，使得机器学习行业的研究者更容易在这个方向上做出贡献（即使是那些不精通差分隐私中的数学的人）。在这篇博文中，我们将展示怎么做到这一点。

最关机的一系列算法被命名为 Private Aggregation of Teacher Ensembles (PATE)。在 PATE 框架中的一个最重要的之处，除了它的名字外，就是每个知道怎么训练一个有监督机器学习模型（比如说一个神经网络）的人都能为机器学习上的差分隐私研究做出贡献。PATE 通过精心协调几个不同的机器学习模型的活动来实现隐私学习。只要你遵循 PATE 框架的的流程，整个模型结果就将有可量化的隐私保证。每个独立的机器模型都是用普通的有监督学习技巧训练的，而我们大多数读者可能已经对怎么攻击 ImageNet 的分类或其他传统的机器学习方向很熟悉了。

如有有人能够在 PATE 下为单个模型设计出更好的架构，或者说更好的训练算法，那么他们也能改进监督学习本身（并不只是个人的分类结果）。实际上，差分隐私可以被看成是一些从业者在普遍遇到的问题上的一个约束，即使在不要求差分隐私的设定下也是这样。这里面就包括过拟合。我们会在这篇博客中陈述在隐私和学习二者之前有趣的联系。特别的是，我们最近提出了扩展的 PATE，其改进了怎么协调不同的机器学习模型，从而能同时提高 PATE 框架得到的模型的准确性和隐私性。这表明了差分隐私的目标和学习模型的目标是一致的。

## 我们为什么需要隐私机器学习算法？

机器学习算法通过研究大量的数据来更新它的参数，并把数据间的关系编码到其中。理想情况下，我们希望这些机器学习的参数是以泛化的形式编码的（例如「吸烟的人更容易患心脏病」），而不是具体在一些特点的训练样本上（例如「Jane Smith」患有心脏病）。不幸的是，在默认情况下，机器学习模型并不会在学习的时候忽略这些（特定的）细节。如果我们希望用机器学习来解决一个重要的问题，比如说做一个癌症诊断模型，那么在我们发布那个机器学习模型的时候（例如，为全世界的医生做个开源的癌症诊断模型），我们也可能无意中透露了有关训练集的信息。恶意的攻击者可以通过检查已发布的模型来得到有关 Jane Smith 的隐私信息 [SSS17]。这就是为什么要提出差分隐私。

## 我们怎么定义和保证隐私？

科学家们已经提出了许多在分析数据的同时提供隐私保障的方法。比如，通过删除一些私人的详细信息，或者用随机的值来替代，都是在数据分析的时候很常用的数据匿名化方法。通常的一些匿名的细节例子包括电话号码和邮政编码。然而，数据匿名话优势并不足以保证隐私，且当攻击者能够得到一些相关的数据集的个体的辅助信息的时候，其能提供的隐私就大打折扣了。众所周知，由于这个原因，研究人员能够 依照个人在互联网电影数据库（IMDb)公开分析的电影评级，对 Netflix 奖的参与者发布的电影评级数据集进行去匿名化 [NS08]。如果 Jane Smith 在 Netflix 奖数据集为电影 A, B, C 打了相同的评级，而且在 IMDb 上公开，那么研究人员可以在两个数据集中把和 Jane 相对应的数据匹配起来。反过来说，这又给他们机会，让他能恢复出 Netflix 奖项中包含但不在 IMDb 中包含的评级。这个例子说明了定义和保障隐私是多么困难，因为攻击者能访问到的个体的信息的范围是很难估计的。

差分隐私是一个框架，其被用来评估那些设计得能保证隐私的机制。它由 Cynthia Dwork，Frank McSherry，Kobbi Nissim 和 Adam Smith [DMNS06] 发明，其解决了 k-anonymity 等先前的方法的许多局限性。差分隐私的基本思路是随机化部分「机制」的行为来提供隐私。在我们的例子里，所谓的「机制」通常指的是一种学习算法，但其实差分隐私框架适用于研究任何算法。

我们所使用的差分隐私的版本要求，在我们改动训练集中的一个训练样本的时候，所学习出来的参数的概率应该大致相同。这就意味着我们可以添加一个训练样本，减少一个训练样本，或者改变其中一个训练样本的的值。直观上感觉就是，一个单个的患者（Jane Smith）不会影响（模型）学习后的输入，因此那个患者的记录也就没被（模型）记录而他的隐私也就得到了保证。在本文接下来的部分，我们把这个概率称为 privacy budget（隐私预算）。越小的 privacy budget 就对应着越大的隐私保证。
![differential-privacy](img/differential-privacy.png)